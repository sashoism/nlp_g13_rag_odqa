{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:42:16.524820Z",
     "start_time": "2024-06-12T11:42:11.484229Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexmano/miniforge3/envs/bcqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from constants import Split\n",
    "from data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "from retriever.Contriever import Contriever\n",
    "from data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from metrics.SimilarityMatch import CosineSimilarity as CosScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:48:34.273183Z",
     "start_time": "2024-06-12T11:42:16.526248Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexmano/miniforge3/envs/bcqa/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 1423640.19it/s]\n",
      "Transforming passage dataset: 100%|██████████| 563424/563424 [00:00<00:00, 632210.41it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/alexmano/miniforge3/envs/bcqa/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harley-Davidson Harley-Davidson\n",
      "KeysView(<Section: Data-Path>)\n",
      "12576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:28<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded of length 12000\n",
      "info:  1200 1200 563424 <data.datastructures.question.Question object at 0x3b84b0c70>\n"
     ]
    }
   ],
   "source": [
    "loader = RetrieverDataset(\"wikimultihopqa\", \"wiki-musiqueqa-corpus\", \"evaluation/config.ini\", Split.DEV)\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "\n",
    "print(\"info: \", len(queries), len(qrels), len(corpus), queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:58:54.516622Z",
     "start_time": "2024-06-12T11:58:42.373138Z"
    }
   },
   "outputs": [],
   "source": [
    "config_instance = DenseHyperParams(query_encoder_path=\"facebook/contriever\",\n",
    "                                   document_encoder_path=\"facebook/contriever\",\n",
    "                                   batch_size=32)\n",
    "tasb_search = Contriever(config_instance)\n",
    "similarity_measure = CosScore()\n",
    "response = tasb_search.retrieve(corpus, queries, 100, similarity_measure)\n",
    "# print(response)\n",
    "print(\"indices\",len(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:58:54.625410Z",
     "start_time": "2024-06-12T11:58:54.518619Z"
    }
   },
   "outputs": [],
   "source": [
    "from metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "\n",
    "metrics = RetrievalMetrics(k_values=[1,5,10])\n",
    "print(metrics.evaluate_retrieval(qrels=qrels, results=response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:03:16.650075Z",
     "start_time": "2024-06-12T12:03:16.637438Z"
    }
   },
   "outputs": [],
   "source": [
    "# Understand queries\n",
    "print(queries[0].id())\n",
    "print(queries[0].text())\n",
    "\n",
    "# Qrels\n",
    "qrel1 = next(iter(qrels.items()))\n",
    "test_query_id, test_context_ids = qrel1\n",
    "\n",
    "print(test_query_id)\n",
    "print(test_context_ids)\n",
    "\n",
    "# Corpus\n",
    "print(corpus[401226].text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(response.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:03:40.539917Z",
     "start_time": "2024-06-12T12:03:40.522565Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def expand_contexts(top_k=5, n_additional_docs=10, strategy=\"top_k_plus_hard_negative_docs\"):\n",
    "    q_contexts = {}\n",
    "    \n",
    "    for q_id, contexts in response.items():\n",
    "        context_ids = list(map(int, contexts.keys()))\n",
    "        q_contexts[q_id] = context_ids[:top_k]\n",
    "        if strategy == \"top_k_plus_random_docs\":\n",
    "            # Sample n_additional_docs random docs. If one of them is already in the top k, sample again\n",
    "            while len(q_contexts[q_id]) < top_k + n_additional_docs:\n",
    "                random_doc = np.random.choice(len(corpus), 1)[0]\n",
    "                if random_doc not in context_ids:\n",
    "                    q_contexts[q_id].append(random_doc)\n",
    "                    \n",
    "        elif strategy == \"top_k_plus_hard_negative_docs\":\n",
    "            # Sample n_additional_docs hard negative docs from context_ids. A hard negative doc is a doc that is relevant to another query\n",
    "            # but not to the current query and it is not present in the current query's ground truth\n",
    "            gt_contexts = list(map(int, qrels[q_id].keys()))\n",
    "            # Make a new list of context_ids that are not in the current query's ground truth. Use filter\n",
    "            context_ids = list(filter(lambda x: x not in gt_contexts, context_ids))\n",
    "            # Concat n_additional_docs hard negative docs\n",
    "            q_contexts[q_id] += context_ids[:n_additional_docs]\n",
    "            \n",
    "    return q_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:03:41.949336Z",
     "start_time": "2024-06-12T12:03:41.860871Z"
    }
   },
   "outputs": [],
   "source": [
    "# strategy = \"top_k_plus_hard_negative_docs\"\n",
    "strategy = \"top_k_plus_hard_negative_docs\"\n",
    "n_additional_docs = 10\n",
    "top_k = 5\n",
    "\n",
    "q_contexts = expand_contexts(top_k, n_additional_docs, strategy)\n",
    "print(q_contexts)\n",
    "# Save q_contexts to a file\n",
    "np.save(\"exp_3_4/{}__top_k{}__add_docs_{}.npy\".format(strategy, top_k, n_additional_docs), q_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:57:23.392317Z",
     "start_time": "2024-06-12T12:57:23.353343Z"
    }
   },
   "outputs": [],
   "source": [
    "q_contexts_hn = q_contexts\n",
    "\n",
    "np.save(\"exp_3_4/{}__top_k{}__add_docs_{}.npy\".format(\"top_k_plus_hard_negative_docs\", top_k, n_additional_docs), q_contexts_hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:47:45.082535Z",
     "start_time": "2024-06-12T12:47:44.651622Z"
    }
   },
   "outputs": [],
   "source": [
    "q_contexts_rd = expand_contexts(top_k, n_additional_docs, \"top_k_plus_random_docs\")\n",
    "\n",
    "np.save(\"exp_3_4/{}__top_k{}__add_docs_{}.npy\".format(\"top_k_plus_random_docs\", top_k, n_additional_docs), q_contexts_rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T13:02:55.731423Z",
     "start_time": "2024-06-12T13:02:53.024082Z"
    }
   },
   "outputs": [],
   "source": [
    "from ollama import generate, create\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "# Prompt construction\n",
    "modelfile = \"\"\"\n",
    "FROM llama3:8b\n",
    "PARAMETER temperature 0.5\n",
    "\"\"\"\n",
    "\n",
    "create(model=\"llama3-custom\", modelfile=modelfile)\n",
    "\n",
    "\n",
    "def create_prompt(question, context_ids):\n",
    "    prompt = \"Please answer the given question based on the given contexts below.\\n\"\n",
    "    i = 1\n",
    "    for id in context_ids:\n",
    "        prompt += f\"Context {i}: \" + corpus[id].text() + \"\\n\"\n",
    "        i += 1\n",
    "    prompt += \"Question: \" + question + \"\\n\"\n",
    "    prompt += \"Constraint: Don't give any explanations and use MAX 5 tokens in your response. No yapping.\\n\"\n",
    "    return prompt\n",
    "\n",
    "def eval(retrived_contexts, test_data):\n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    results = []\n",
    "    exact_matches = 0\n",
    "    ground_truth = \"\"\n",
    "    question = \"\"\n",
    "    i = 0\n",
    "    for question_id, contexts_list in tqdm.tqdm(retrived_contexts.items()):\n",
    "        context_ids = contexts_list\n",
    "    \n",
    "        for que in test_data:\n",
    "            if que[\"_id\"] == question_id:\n",
    "                question = que[\"question\"]\n",
    "                ground_truth = que[\"answer\"]\n",
    "        prompt = create_prompt(question, context_ids)\n",
    "        prediction = generate(model=\"llama3-custom\", prompt=prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"q_id\": question_id, \n",
    "            \"prediction\": prediction[\"response\"], \n",
    "            \"ground_truth\": ground_truth\n",
    "        })\n",
    "        \n",
    "        if prediction[\"response\"] == ground_truth:\n",
    "            exact_matches += 1\n",
    "        \n",
    "        # if i == 5:\n",
    "        #     return results\n",
    "        i+=1\n",
    "            \n",
    "    accuracy = exact_matches / len(test_data)\n",
    "    results_info = {\"exact_matches\": exact_matches, \"accuracy\": accuracy}\n",
    "    # with open(f\"top_{top_k}.json\", \"w\") as f:\n",
    "    #     json.dump(results_info, f)\n",
    "    print(results_info)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the first 10 contexts for each query in q_contexts_hn\n",
    "q_contexts_hn_10 = {k: v[:13] for k, v in q_contexts_hn.items()}\n",
    "q_contexts_rd_10 = {k: v[:13] for k, v in q_contexts_rd.items()}\n",
    "\n",
    "# Save\n",
    "np.save(\"exp_3_4/top_k_plus_hard_negative_docs__top_k5__add_docs_8.npy\", q_contexts_hn_10)\n",
    "np.save(\"exp_3_4/top_k_plus_random_docs__top_k5__add_docs_8.npy\", q_contexts_rd_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T13:04:07.059014Z",
     "start_time": "2024-06-12T13:02:56.801122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 146/1200 [03:58<1:04:03,  3.65s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open(\"dataset/data/dev.json\", \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "# with open(\"dataset/wiki_musique_corpus.json\", \"r\") as file:\n",
    "#     context_data = json.load(file)\n",
    "\n",
    "# with open(\"context_indices.json\", \"r\") as file:\n",
    "#     context_ids = json.load(file)\n",
    "\n",
    "# Load q_contexts_hn\n",
    "q_contexts_hn = np.load(\"exp_3_4/top_k_plus_hard_negative_docs__top_k5__add_docs_2.npy\", allow_pickle=True).item()\n",
    "\n",
    "# Load q_contexts_rd\n",
    "q_contexts_rd = np.load(\"exp_3_4/top_k_plus_random_docs__top_k5__add_docs_2.npy\", allow_pickle=True).item()\n",
    "\n",
    "eval_results = eval(q_contexts_hn, test_data)\n",
    "# eval(context_ids, context_data, test_data, 7)\n",
    "# print(next(iter(context_ids.items())))\n",
    "\n",
    "# type(q_contexts_hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T13:04:10.528357Z",
     "start_time": "2024-06-12T13:04:10.510681Z"
    }
   },
   "outputs": [],
   "source": [
    "print(eval_results)\n",
    "# Save eval_results to a file\n",
    "\n",
    "with open(\"exp_3_4/eval_results_top_k_plus_hard_negative_docs__top_k5__add_docs_2.json\", \"w\") as f:\n",
    "    json.dump(eval_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "def calculate_metrics(prediction, ground_truth):\n",
    "    prediction_tokens = re.findall(r\"\\w+\", prediction.lower())\n",
    "    ground_truth_tokens = re.findall(r\"\\w+\", ground_truth.lower())\n",
    "\n",
    "    common_tokens = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_common_tokens = sum(common_tokens.values())\n",
    "\n",
    "    if len(prediction_tokens) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = num_common_tokens / len(prediction_tokens)\n",
    "\n",
    "    if len(ground_truth_tokens) == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = num_common_tokens / len(ground_truth_tokens)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load eval_results from exp_3_4 folder\n",
    "with open(\"exp_3_4/eval_results_top_k_plus_hard_negative_docs__top_k5__add_docs_10.json\", \"r\") as f:\n",
    "    eval_results_hn = json.load(f)\n",
    "\n",
    "# Load eval_results from exp_3_4 folder\n",
    "with open(\"exp_3_4/eval_results_top_k_plus_random_docs__top_k5__add_docs_10.json\", \"r\") as f:\n",
    "    eval_results_rd = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of predictions and ground truths\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "for result in eval_results_hn:\n",
    "    predictions.append(result[\"prediction\"])\n",
    "    ground_truths.append(result[\"ground_truth\"])\n",
    "\n",
    "avg_exact_match = exact_match.compute(\n",
    "    predictions=predictions,\n",
    "    references=ground_truths,\n",
    "    ignore_case=True,\n",
    "    ignore_punctuation=True,\n",
    ")\n",
    "\n",
    "precisions, recalls, f1s = [], [], []\n",
    "for i in range(len(predictions)):\n",
    "    precision, recall, f1 = calculate_metrics(predictions[i], ground_truths[i])\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "\n",
    "avg_precision = sum(precisions) / len(precisions)\n",
    "avg_recall = sum(recalls) / len(recalls)\n",
    "avg_f1 = sum(f1s) / len(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"avg_exact_match\": avg_exact_match[\"exact_match\"],\n",
    "    \"avg_precision\": avg_precision,\n",
    "    \"avg_recall\": avg_recall,\n",
    "    \"avg_f1\": avg_f1,\n",
    "}\n",
    "\n",
    "with open(\"exp_3_4/metrics_top_k_plus_hard_negative_docs__top_k5__add_docs_10.json\", \"w\") as fp:\n",
    "    json.dump(metrics, fp)\n",
    "\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
